## 1. 上传及配置环境变量
```shell
cd /opt/software/
#上传
tar -zxvf /opt/software/software/* -C /opt/module/
sudo vim /etc/profile.d/my_env.sh

#写入
#java 
export JAVA_HOME=/opt/module/jdk1.8.0_212 
export PATH=$PATH:$JAVA_HOME/bin

#hadoop 
export HADOOP_HOME=/opt/module/hadoop-3.1.3 
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

#zookeeper
export ZOOKEEPER_HOME=/opt/module/zookeeper-3.5.7
export PATH=$PATH:$ZOOKEEPER_HOME/bin

#kafka
export KAFKA_HOME=/opt/module/kafka_2.11-2.4.1
export PATH=$PATH:$KAFKA_HOME/bin

#flume
export FLUME_HOME=/opt/module/flume-1.9.0
export PATH=$PATH:$FLUME_HOME/bin

#运行自动配置脚本
============================================================

```
## 2. 分发之前
```shell
#zook
vim /opt/module/zookeeper-3.5.7/conf/zoo.cfg

#将
dataDir=/tmp/zookeeper
#修改为
dataDir=/opt/module/zookeeper-3.5.7/zkData
#在最下面添加
server.2=hadoop102:2888:3888
server.3=hadoop103:2888:3888
server.4=hadoop104:2888:3888


#kafka
vim /opt/module/kafka_2.11-2.4.1/config/server.properties

---------------------------------------------------
#修改
broker.id=0
#为
broker.id=2  #这里2和zookeeper那里的一样
---------------------------------------------------
#修改
log.dirs=/tmp/kafka-logs
#为
log.dirs=/opt/module/kafka_2.11-2.4.1/datas
---------------------------------------------------
#修改
zookeeper.connect=localhost:9092
#为
zookeeper.connect=hadoop102:9092,hadoop103:9092,hadoop104:9092/kafka

#Flume
cd /opt/module/flume-1.9.0/lib
ll | grep guava
rm -rf guava-11.0.2.jar

#断开重连
cd /opt/module/flume-1.9.0
mkdir jobs
```
## 3. 分发
```shell
xsync /opt/module/*
#分发环境配置
scp /etc/profile.d/my_env.sh root@hadoop103:/etc/profile.d/
scp /etc/profile.d/my_env.sh root@hadoop104:/etc/profile.d/
```

## 4. hadoop
```shell

sudo /home/wuss/bin/xsync /etc/profile.d/my_env.sh  
xcall.sh hadoop version

#第一次启动
hdfs namenode -format
#测试yran

cd /opt/module/hadoop-3.1.3/share/hadoop/mapreduce
hadoop jar hadoop-mapreduce-examples-3.1.3.jar pi 5 10
```
## 5. zookeeper
```shell

#在hadoop103和104
vim /opt/module/zookeeper-3.5.7/zkData/myid

#hadoop103写3,每台机不同
3
#hadoop104写4,每台机不同
4


#断开重连
#启动
zkServer.sh start
#查看状态
zkServer.sh status

#进入
zkCli.sh
#查看
ls /
#退出
quit
```
## 6. kafka
```shell

#断开重连
vim /opt/module/kafka_2.11-2.4.1/config/server.properties

#hadoop103写3,每台机不同
broker.id=3
# 6. #hadoop104写4,每台机不同
broker.id=4

#启动之前必须启动zookeeper
#启动
kafka-server-start.sh -daemon /opt/module/kafka_2.11-2.4.1/config/server.properties
#停止
kafka-server-stop.sh
```