## 1. 上传
```shell
cd /opt/software/
上传datax.tar.gz
tar -zxvf datax.tar.gz -C /opt/module/
cd /opt/module/datax/
python bin/datax.py job/job.json
```
## 2. MySQLReader+HDFSWriter配置样例
- TableMode：
```shell
vim job/base_province.json
```
```json
{  
  "job": {  
    "setting": {  
      "speed": {  
        "channel": 3  
      },  
      "errorLimit": {  
        "record": 0,  
        "percentage": 0.02  
      }    },  
    "content": [  
      {        "reader": {  
          "name": "mysqlreader",  
          "parameter": {  
            "username": "root",  
            "password": "123456",  
            "column": [  
              "id",  
              "name",  
              "region_id",  
              "area_code",  
              "iso_code",  
              "iso_3166_2"  
            ],  
            "splitPk": "id",  
            "connection": [  
              {                "table": [  
                  "base_province"  
                ],  
                "jdbcUrl": [  
                  "jdbc:mysql://hadoop102:3306/gmall"  
                ]  
              }            ]          }        },  
        "writer": {  
          "name": "hdfswriter",  
          "parameter": {  
            "column": [  
              {                "name": "id",  
                "type": "bigint"  
              },  
              {  
                "name": "name",  
                "type": "string"  
              },  
              {  
                "name": "region_id",  
                "type": "string"  
              },  
              {  
                "name": "area_code",  
                "type": "string"  
              },  
              {  
                "name": "iso_code",  
                "type": "string"  
              },  
              {  
                "name": "iso_3166_2",  
                "type": "string"  
              }  
            ],  
            "compress": "gzip",  
            "defaultFS": "hdfs://hadoop102:8020",  
            "fieldDelimiter": "\t",  
            "fileName": "base_province",  
            "fileType": "text",  
            "path": "/base_province",  
            "writeMode": "append"  
          }  
        }      }    ],  
    "setting": {  
      "speed": {  
        "channel": 1  
      }    }  }}
```
```shell
cd /opt/module/datax/
hadoop fs -mkdir /base_province
python bin/datax.py job/base_province.json
```
用hive查看数据库
启动hive
```sql
CREATE EXTERNAL TABLE base_province
(
  `id`         STRING COMMENT '编号',
  `name`       STRING COMMENT '省份名称',
  `region_id`  string comment '地区ID',
  `area_code`  string comment '地区编码',
  `iso_code`   string comment '旧版ISO-3166-2编码,供可视化使用',
  `iso_3166_2` string comment '新版ISO-3166-2编码,供可视化使用'
) comment '省份表'
    row format delimited fields terminated by '\\t'
    NULL DEFINED AS ''
    location '/base_province/';

select * from base_province;
```
- QuerySQLMode
```shell
cd /opt/module/datax/
vim job/base_province.json
```
```json
{  
  "job": {  
    "content": [  
      {        "reader": {  
          "name": "mysqlreader",  
          "parameter": {  
            "connection": [  
              {                "jdbcUrl": [  
                  "jdbc:mysql://hadoop102:3306/gmall"  
                ],  
                "querySql": [  
                  "select id,name,region_id,area_code,iso_code,iso_3166_2 from base_province where id>=3"  
                ]  
              }            ],  
            "password": "123456",  
            "username": "root"  
          }  
        },  
        "writer": {  
          "name": "hdfswriter",  
          "parameter": {  
            "column": [  
              {                "name": "id",  
                "type": "bigint"  
              },  
              {  
                "name": "name",  
                "type": "string"  
              },  
              {  
                "name": "region_id",  
                "type": "string"  
              },  
              {  
                "name": "area_code",  
                "type": "string"  
              },  
              {  
                "name": "iso_code",  
                "type": "string"  
              },  
              {  
                "name": "iso_3166_2",  
                "type": "string"  
              }  
            ],  
            "compress": "gzip",  
            "defaultFS": "hdfs://hadoop102:8020",  
            "fieldDelimiter": "\t",  
            "fileName": "base_province",  
            "fileType": "text",  
            "path": "/base_province",  
            "writeMode": "append"  
          }  
        }      }    ],  
    "setting": {  
      "speed": {  
        "channel": 1  
      }    }  }}
```
```shell
cd /opt/module/datax/
hadoop fs -mkdir /base_province
python bin/datax.py job/base_province.json
hadoop fs -cat /base_province/* | zcat
```
## 3. 动态传参
```shell
json中
"path": "/base_province/${dt}",

创建目标路径
hadoop fs -mkdir /base_province/2020-06-14

进入DataX根目录
cd /opt/module/datax

执行如下命令
python bin/datax.py -p"-Ddt=2020-06-14" job/base_province.json
```
## 4. HDFSReader+MySQLWriter配置样例
```shell
cd /opt/module/datax
vim job/base_province.json
```
```json
{  
  "job": {  
    "content": [  
      {        "reader": {  
          "name": "hdfsreader",  
          "parameter": {  
            "defaultFS": "hdfs://hadoop102:8020",  
            "path": "/base_province",  
            "column": [  
              "*"  
            ],  
            "fileType": "text",  
            "compress": "gzip",  
            "encoding": "UTF-8",  
            "nullFormat": "\\N",  
            "fieldDelimiter": "\t"  
          }  
        },  
        "writer": {  
          "name": "mysqlwriter",  
          "parameter": {  
            "username": "root",  
            "password": "123456",  
            "connection": [  
              {                "table": [  
                  "test_province"  
                ],  
                "jdbcUrl": "jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&characterEncoding=utf-8"  
              }  
            ],  
            "column": [  
              "id",  
              "name",  
              "region_id",  
              "area_code",  
              "iso_code",  
              "iso_3166_2"  
            ],  
            "writeMode": "replace"  
          }  
        }      }    ],  
    "setting": {  
      "speed": {  
        "channel": 1  
      }    }  }}
```
在MySQL中创建gmall.test_province表
```sql
mysql -uroot -p123456

USE gmall;

CREATE TABLE `test_province` (
`id` bigint(20) NOT NULL,
`name` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
`region_id` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
`area_code` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
`iso_code` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
`iso_3166_2` varchar(20) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
PRIMARY KEY (`id`)
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci ROW_FORMAT = Dynamic;
```
```shell
python bin/datax.py job/base_province.json
```
