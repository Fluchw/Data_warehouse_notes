
## 1. 安装
```shell
cd /opt/software/
#上传apache-flume-1.9.0-bin.tar.gz
tar -zxvf apache-flume-1.9.0-bin.tar.gz -C ../module/
cd /opt/module
mv apache-flume-1.9.0-bin/ flume
cd /opt/module/flume/lib
ll | grep guava
rm -rf guava-11.0.2.jar
sudo vim /etc/profile.d/my_env.sh

三台都要添加
#flume
export FLUME_HOME=/opt/module/flume
export PATH=$PATH:$FLUME_HOME/bin

#断开重连
cd /opt/module/flume/conf
vim log4j.properties
flume.log.dir=/opt/module/flume/logs
cd /opt/module/flume
mkdir job
```
## 2. file_to_kafka.conf
```shell
cd job
vim file_to_kafka.conf
------------------------------------------------------------------
a1.sources = r1
a1.channels = c1

a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*

# 断点续传的检查点目录
a1.sources.r1.positionFile= /opt/module/flume/taildir_position.json

# 拦截器(type通过下图获取)
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type=flume.interceptor.ETLInterceptor$Builder

a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092
a1.channels.c1.kafka.topic=topic_log

a1.channels.c1.parseAsFlumeEvent=false

a1.sources.r1.channels = c1
--------------------------------------------------------------------
```
## 3. 写拦截器
![[数仓采集/数仓截图/28.png)
pom.xml
```xml
<?xml version="1.0" encoding="UTF-8"?>  
<project xmlns="http://maven.apache.org/POM/4.0.0"  
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"  
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">  
    <modelVersion>4.0.0</modelVersion>  
  
    <groupId>org.example</groupId>  
    <artifactId>collect</artifactId>  
    <version>1.0-SNAPSHOT</version>  
  
    <properties>        <maven.compiler.source>8</maven.compiler.source>  
        <maven.compiler.target>8</maven.compiler.target>  
    </properties>  
  
    <dependencies>        <dependency>  
            <groupId>org.apache.flume</groupId>  
            <artifactId>flume-ng-core</artifactId>  
            <version>1.9.0</version>
            <scope>provided</scope>  
        </dependency>  
  
        <dependency>  
            <groupId>com.alibaba</groupId>  
            <artifactId>fastjson</artifactId>  
            <version>1.2.62</version>  
        </dependency>  
    </dependencies>  
        <build>  
        <plugins>  
            <plugin>  
                <artifactId>maven-compiler-plugin</artifactId>  
                <version>2.3.2</version>  
                <configuration>  
                    <source>1.8</source>  
                    <target>1.8</target>  
                </configuration>  
            </plugin>  
            <plugin>  
                <artifactId>maven-assembly-plugin</artifactId>  
                <configuration>  
                    <descriptorRefs>  
                        <descriptorRef>jar-with-dependencies</descriptorRef>  
                    </descriptorRefs>  
                </configuration>  
                <executions>  
                    <execution>  
                        <id>make-assembly</id>  
                        <phase>package</phase>  
                        <goals>                            
	                        <goal>single</goal>  
                        </goals>  
                    </execution>  
                </executions>  
            </plugin>  
        </plugins>  
    </build>  
  
</project>
```
![[数仓采集/数仓截图/29.png)
ETLInterceptor
```java
package flume.interceptor;  
  
import com.alibaba.fastjson.JSONException;  
import com.alibaba.fastjson.JSONObject;  
import org.apache.flume.Context;  
import org.apache.flume.Event;  
import org.apache.flume.interceptor.Interceptor;  
  
import java.nio.charset.StandardCharsets;  
import java.util.Iterator;  
import java.util.List;  
  
/**  
 * 1.实现接口org.apache.flume.interceptor.Interceptor  
 * 2.实现4个抽象方法  
 * 3.静态内部类builder实现接口Builder  
 * */  
public class ETLInterceptor implements Interceptor{  
    @Override  
    public void initialize() {  
  
    }  
    @Override  
    public Event intercept(Event event) {  
//        需求:过滤event中的数据是否是json格式  
        byte[] body = event.getBody();  
        String log = new String(body, StandardCharsets.UTF_8);  
  
        boolean flag=JSONUtils.isJson(log);  
        return flag ? event : null;  
    }  
  
    @Override  
    public List<Event> intercept(List<Event> list) {  
        //  将处理过之后为null的event删除掉  
        //  使用迭代器  
        Iterator<Event> iterator = list.iterator();  
        while (iterator.hasNext()){  
            Event event = iterator.next();  
            if (intercept(event) == null) {  
                iterator.remove();  
            }  
        }  
  
        return list;  
    }  
  
    @Override  
    public void close() {  
  
    }  
    public static class Builder implements Interceptor.Builder{  
  
        @Override  
        public Interceptor build() {  
            return new ETLInterceptor();  
        }  
  
        @Override  
        public void configure(Context context) {  
  
        }    }  
}
```
JSONUtils
```java
package flume.interceptor;  
  
import com.alibaba.fastjson.JSONException;  
import com.alibaba.fastjson.JSONObject;  
  
public class JSONUtils {  
    public static boolean isJson(String log){  
        boolean flag = false;  
        //  判断log是否是json  
        try{  
            JSONObject.parseObject(log);  
            flag = true;  
        }catch (JSONException e){  
  
        }  
        return flag;  
    }  
}
```
## 4. 上传
```shell
cd /opt/module/flume/lib
上传collect-1.0-SNAPSHOT-jar-with-dependencies.jar

分发
cd /opt/module
xsync /opt/module/flume
```
## 5. 测试
```shell
在hadoop102
cd /opt/module/flume
bin/flume-ng agent -n a1 -c conf/ -f job/file_to_kafka.conf -Dflume.root.logger=info,console

在hadoop103
cd /opt/module/kafka_2.11-2.4.1
bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic topic_log

在hadoop102
log.sh
```
## 6. 修改配置
```shell
cd /opt/module/flume/job
vim file_to_kafka.conf

--------------------------------------------------------------------------------------------------------
a1.sources = r1
a1.channels = c1

a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*

# 断点续传的检查点目录
a1.sources.r1.positionFile= /opt/module/flume/taildir_position.json

# 拦截器(type通过下图获取)
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type=flume.interceptor.ETLInterceptor$Builder

a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092
a1.channels.c1.kafka.topic=topic_log

a1.channels.c1.parseAsFlumeEvent=false

a1.sources.r1.channels = c1
--------------------------------------------------------------------------------------------------------
```
![image](数仓截图2/29.jpg]]
## 7. 启动
```shell
创建消费者组
kafka-topics.sh --bootstrap-server hadoop102:9092 --create --partitions 3 --topic topic_log

hadoop103
kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic topic_log

hadoop102
cd /opt/module/flume/
bin/flume-ng agent -c conf/ -f job/file_to_kafka.conf -n a1 -Dflume.root.logger=INFO,console

新生成数据,仍然可以传输
log.sh

cd /opt/module/applog/log
正常输入以下,可以在hadoop103接收到
echo '{id:1}' >> app.2022-09-16.log
错误输入以下,在hadoop103无接收
echo '{id:' >> app.2022-09-16.log

分发
xsync /opt/module/flume/job/
```
## 8. flume脚本
```shell
cd /home/wuss/bin
vim f1.sh

#!/bin/bash
case $1 in
"start"){
 for i in hadoop102 hadoop103
 do
 echo " --------启动 $i 采集flume-------"
 ssh $i "nohup /opt/module/flume/bin/flume-ng agent -c conf/ -f /opt/module/flume/job/file_to_kafka.conf -n a1 -c /opt/module/flume/conf/ -f /opt/module/flume/job/file_to_kafka.conf >/dev/null 2>&1 &"
 done
};;

"stop"){
 for i in hadoop102 hadoop103
 do
 echo " --------停止 $i 采集flume-------"
 ssh $i "ps -ef | grep file_to_kafka.conf | grep -v grep | awk '{print \$2}' | xargs -n1 kill -9"
 done
};;
esac
```
## 9. 将kafka的数据消费到hdfs
```shell
cd /opt/module/flume/job
vim kafka_to_hdfs.conf

---------------------------------------------------------------------------
a1.sources= r1
a1.channels= c1
a1.sinks= k1

# sources
a1.sources.r1.channels=c1
a1.sources.r1.type=org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092
a1.sources.r1.kafka.consumer.group.id=flume2
a1.sources.r1.kafka.topics=topic_log
a1.sources.r1.batchSize= 5000
a1.sources.r1.batchDurationMillis=2000

# channels
a1.channels.c1.type= file
a1.channels.c1.dataDirs= /opt/module/flume/data/behavior1
a1.channels.c1.checkpointDir= /opt/module/flume/checkpoint/behavior1

# sinks
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix= log-
a1.sinks.k1.hdfs.rollInterval= 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.fileType= CompressedStream
a1.sinks.k1.hdfs.codeC=gzip

## 拼装

a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1
---------------------------------------------------------------------------
xsync kafka_to_hdfs.conf
```
在hadoop102
```shell
f1.sh start
myhadoop.sh start
```
在hadoop103
```shell
kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic topic_log
```
在hadoop104上
```shell
cd /opt/module/flume

/opt/module/flume/bin/flume-ng agent -c conf/ -f job/kafka_to_hdfs.conf -n a1 -Dflume.root.logger=INFO,console
```
此时进入 [Browsing HDFS](http://hadoop102:9870/explorer.html#/) 发现没有数据,因为kafka只能消费新生产的数据

hadoop102
```shell
log.sh
```
然后发现该页面多了个  origin_data  文件夹,即为上传的数据
## 10. flume解决零点漂移的拦截器
```java
package flume.interceptor;  
  
import com.alibaba.fastjson.JSONObject;  
import org.apache.flume.Context;  
import org.apache.flume.Event;  
import org.apache.flume.interceptor.Interceptor;  
  
import java.nio.charset.StandardCharsets;  
import java.util.List;  
import java.util.Map;  
  
public class TimestampInterceptor implements Interceptor {  
    @Override  
    public void initialize() {  
  
    }  
    @Override  
    public Event intercept(Event event) {  
        // 需求: 修改headers中的时间戳值  改为日志数据中的时间  
        // 提供给hdfs sink使用 控制输出文件的文件夹名称  
        byte[] body = event.getBody();  
        String log = new String(body, StandardCharsets.UTF_8);  
  
        JSONObject jsonObject = JSONObject.parseObject(log);  
        String timestamp = jsonObject.getString("ts");  
  
        // 获取headers  
        Map<String,String> headers = event.getHeaders();  
        headers.put("timestamp",timestamp);  
  
        return event;  
    }  
  
    @Override  
    public List<Event> intercept(List<Event> list) {  
        for (Event event : list) {  
            intercept(event);  
        }  
        return list;  
    }  
  
    @Override  
    public void close() {  
  
    }  
    public static class Builder implements Interceptor.Builder{  
        @Override  
        public Interceptor build() {  
            return new TimestampInterceptor();  
        }  
  
        @Override  
        public void configure(Context context) {  
  
        }    }  
}
```
```shell
上传到
cd /opt/module/flume/lib/
xsync collect-1.0-SNAPSHOT-jar-with-dependencies.jar

cd /opt/module/flume/job
vim kafka_to_hdfs.conf

添加拦截器
--------------------------------------------------
a1.sources= r1
a1.channels= c1
a1.sinks= k1

a1.sources.r1.channels=c1
a1.sources.r1.type=org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092
a1.sources.r1.kafka.consumer.group.id=flume2
a1.sources.r1.kafka.topics=topic_log
a1.sources.r1.batchSize= 5000
a1.sources.r1.batchDurationMillis=2000

a1.sources.r1.interceptors=i1
a1.sources.r1.interceptors.i1.type=flume.interceptor.TimestampInterceptor$Builder

a1.channels.c1.type= file
a1.channels.c1.dataDirs= /opt/module/flume/data/behavior1
a1.channels.c1.checkpointDir= /opt/module/flume/checkpoint/behavior1

a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix= log-
a1.sinks.k1.hdfs.rollInterval= 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.fileType= CompressedStream
a1.sinks.k1.hdfs.codeC=gzip

a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1

--------------------------------------------------
xsync kafka_to_hdfs.conf

```
在hadoop102
```shell
f1.sh start
myhadoop.sh start
```
在hadoop103
```shell
kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic topic_log
```
在hadoop104上
```shell
cd /opt/module/flume

bin/flume-ng agent -c conf/ -f job/kafka_to_hdfs.conf -n a1 -Dflume.root.logger=INFO,console
```
## 11. 消费脚本
```shell
cd /home/wuss/bin
vim f2.sh

#!/bin/bash
case $1 in
"start")
 echo " --------启动 hadoop104 日志数据flume-------"
 echo " --------hadoop104-------"  
 ssh hadoop104 "nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf -f /opt/module/flume/job/kafka_to_hdfs.conf >/dev/null 2>&1 &"
;;

"stop")
 echo " --------停止消费日志flume-------"
 echo " --------hadoop104-------"  
 ssh hadoop104 "ps -ef | grep kafka_to_hdfs.conf | grep -v grep | awk '{print \$2}' | xargs -n1 kill -9"
;;
esac

chmod 777 f2.sh
```
## 12. 启动
hadoop102
```shell
删除掉hdfs里的origin_data
myhadoop.sh start
zk.sh start
kafka.sh start
f1.sh start
f2.sh start
```
在hadoop103
```shell
kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic topic_log
```
查看错误日志
```
若103没有消费到数据,
到hadoop102上
cd /opt/module/flume/logs
查看flume.log

若103没问题,在hdfs上没看到origin_data,
到hadoop104上
cd /opt/module/flume/logs
查看flume.log
```
修改假数据日期
```
cd /opt/module/applog/
vim application.yml
xsync application.yml
log.sh
```

