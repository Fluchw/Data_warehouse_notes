## 1. 修改Maxwell配置文件
```shell
cd /opt/module/maxwell
vim config.properties

添加
-----------------------------------------------------------------
kafka_topic=%{table}

#表过滤,只同步特定13张表
filter=include:gmall.cart_info,include:gmall.comment_info,include:gmall.coupon_use,include:gmall.favor_info,include:gmall.order_detail,include:gmall.order_detail_activity,include:gmall.order_detail_coupon,include:gmall.order_info,include:gmall.order_refund_info,include:gmall.order_status_log,include:gmall.payment_info,include:gmall.refund_payment,include:gmall.user_info
----------------------------------------------------------------
myhadoop.sh start
mxw.sh start
zk.sh start
kafka.sh start
cd /opt/module/db_log/
java -jar gmall2020-mock-db-2021-11-14.jar

在hadoop103
kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic cart_info --from-beginning
```
## 2. Flume时间戳拦截器
```java
新建db软件包
在里面新建TimestampInterceptor类

package flume.interceptor.db;  
  
import com.alibaba.fastjson.JSONObject;  
import org.apache.flume.Context;  
import org.apache.flume.Event;  
import org.apache.flume.interceptor.Interceptor;  
  
import java.nio.charset.StandardCharsets;  
import java.util.List;  
import java.util.Map;  
  
public class TimestampInterceptor implements Interceptor {  
    @Override  
    public void initialize() {  
  
    }  
    @Override  
    public Event intercept(Event event) {  
        // 需求: 将event里数据的时间戳拿出来  
        // 放入到headers中 提供给hdfs sink 控制输出文件路径  
        byte[] body = event.getBody();  
        String log =new String(body,StandardCharsets.UTF_8);  
  
        JSONObject jsonObject =JSONObject.parseObject(log);  
  
        String ts = jsonObject.getString("ts") + "000";  
  
        Map<String,String> headers = event.getHeaders();  
        headers.put("timestamp",ts);  
        return event;  
    }  
  
    @Override  
    public List<Event> intercept(List<Event> list) {  
        for (Event event : list) {  
            intercept(event);  
        }  
  
        return list;  
    }  
  
    @Override  
    public void close() {  
  
    }  
    public static class Builder implements Interceptor.Builder{  
        @Override  
        public Interceptor build() {  
            return new TimestampInterceptor();  
        }  
  
        @Override  
        public void configure(Context context) {  
  
        }    }  
}
```
```shell
上传到hadoop104
cd /opt/module/flume/lib/
```

## 3. 修改Flume配置
```shell
在hadoop104
cd /opt/module/flume/job/
vim kafka_to_hdfs_db.conf
---------------------------------------------------------------------------
a1.sources = r1
a1.channels = c1
a1.sinks = k1

# sources
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092
a1.sources.r1.kafka.topics = cart_info,comment_info,coupon_use,favor_info,order_detail_activity,order_detail_coupon,order_detail,order_info,order_refund_info,order_status_log,payment_info,refund_payment,user_info
a1.sources.r1.kafka.consumer.group.id = flume
a1.sources.r1.setTopicHeader = true
a1.sources.r1.topicHeader = topic
a1.sources.r1.interceptors = i1

#拦截器
a1.sources.r1.interceptors.i1.type=flume.interceptor.db.TimestampInterceptor$Builder

# channels
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume/checkpoint/behavior2
a1.channels.c1.dataDirs = /opt/module/flume/data/behavior2/
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1123456
a1.channels.c1.keep-alive = 6

# sinks
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/db/%{topic}_inc/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = db_
a1.sinks.k1.hdfs.round = false
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = gzip

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1
---------------------------------------------------------------------------

xsync kafka_to_hdfs_db.conf
在hadoop104
cd /opt/module/flume/
bin/flume-ng agent -c conf/ -f job/kafka_to_hdfs_db.conf -n a1 -Dflume.root.logger=info,console

在hadoop102
cd /opt/module/db_log/
java -jar gmall2020-mock-db-2021-11-14.jar

在hadoop102:9870中可以查看/origin_data/gmall/db有28个
```
## 4. 修改数据更新时间
```shell
cd /opt/module/maxwell/
vim config.properties

添加
#该日期须和/opt/module/db_log/application.properties中的mock.date参数保持一致
mock_date=2020-06-14

mxw.sh restart
cd /opt/module/db_log/
java -jar gmall2020-mock-db-2021-11-14.jar
```
## 5. Flume启停脚本
```shell
cd ~/bin
vim f3.sh

(切换到阅读模式再复制,编辑模式的有问题)
-------------------------------------------------------------------------------------
#!/bin/bash
case $1 in
"start")
 echo " --------启动 hadoop104 业务数据flume-------"
 ssh hadoop104 "nohup /opt/module/flume/bin/flume-ng agent -n a1 -c /opt/module/flume/conf -f /opt/module/flume/job/kafka_to_hdfs_db.conf >/dev/null 2>&1 &"
;;
"stop")
 echo " --------停止 hadoop104 业务数据flume-------"
 ssh hadoop104 "ps -ef | grep kafka_to_hdfs_db.conf | grep -v grep |awk '{print \$2}' | xargs -n1 kill"
;;
esac
--------------------------------------------------------------------

chmod 777 f3.sh
```
## 6. 增量同步脚本
```shell
cd ~/bin
vim mysql_to_kafka_inc_init.sh
---------------------------------------------------------
#!/bin/bash
# 该脚本的作用是初始化所有的增量表，只需执行一次
MAXWELL_HOME=/opt/module/maxwell
import_data() {
 $MAXWELL_HOME/bin/maxwell-bootstrap --database gmall --table $1 --config $MAXWELL_HOME/config.properties
}
case $1 in
"cart_info")
 import_data cart_info
 ;;
"comment_info")
 import_data comment_info
 ;;
"coupon_use")
 import_data coupon_use
 ;;
"favor_info")
 import_data favor_info
 ;;
"order_detail")
 import_data order_detail
 ;;
"order_detail_activity")
 import_data order_detail_activity
 ;;
"order_detail_coupon")
 import_data order_detail_coupon
 ;;
"order_info")
 import_data order_info
 ;;
"order_refund_info")
 import_data order_refund_info
 ;;
"order_status_log")
 import_data order_status_log
 ;;
"payment_info")
 import_data payment_info
 ;;
"refund_payment")
 import_data refund_payment
 ;;
"user_info")
 import_data user_info
 ;;
"all")
 import_data cart_info
 import_data comment_info
 import_data coupon_use
 import_data favor_info
 import_data order_detail
 import_data order_detail_activity
 import_data order_detail_coupon
 import_data order_info
 import_data order_refund_info
 import_data order_status_log
 import_data payment_info
 import_data refund_payment
 import_data user_info
 ;;
esac
-------------------------------------------------------
chmod +x mysql_to_kafka_inc_init.sh
f3.sh start
mysql_to_kafka_inc_init.sh all
```