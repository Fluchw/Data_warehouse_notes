## 1. 安装spark
```shell
cd /opt/software/
上传spark-3.0.0-bin-hadoop3.2.tgz和spark-3.0.0-bin-without-hadoop.tgz

tar -zxvf /opt/software/spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
cd /opt/module
mv spark-3.0.0-bin-hadoop3.2 spark

```
## 2. 配置
```shell
sudo vim /etc/profile.d/my_env.sh
#Spark_HOME
export SPARK_HOME=/opt/module/spark
export PATH=$PATH:$SPARK_HOME/bin

断开重连,使环境变量生效
cd /opt/module/hive/conf/
vim spark_defaults.conf
--------------------------------------------------------------------
spark.master                    yarn
spark.eventLog.enabled          true
spark.eventLog.dir              hafs://hadoop102:8020/spark-history
spark.executor.memory           1g
spark.driver.memory             1g
--------------------------------------------------------------------
myhadoop.sh start
hadoop fs -mkdir /spark-history
```
## 3. 上传纯净版spark到hdfs
```shell
cd /opt/software/
tar -zxvf /opt/software/spark-3.0.0-bin-without-hadoop.tgz
cd /opt/software/spark-3.0.0-bin-without-hadoop/jars
hadoop fs -mkdir /spark-jars
hadoop fs -put /opt/software/spark-3.0.0-bin-without-hadoop/jars/* /spark-jars
cd /opt/module/hive/conf/
vim hive-site.xml
------------------------------------------------------------
<!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）-->
<property>
	<name>spark.yarn.jars</name>
	<value>hdfs://hadoop102:8020/spark-jars/*</value>
</property>
<!--Hive执行引擎-->
<property>
	<name>hive.execution.engine</name>
	<value>spark</value>
</property>
<property> 
	<name>hive.spark.client.connect.timeout</name> 
	<value>1000000ms</value> 
</property>    
<property>
  <name>spark.home</name>
  <value>/opt/module/spark</value>
</property>
<property>
<name>hive.users.in.admin.role</name>
<value>hdfs</value>
</property>
--------------------------------------------------------------
cd /opt/module/spark/conf
mv spark-env.sh.template spark-env.sh
vim spark-env.sh
export SPARK_DIST_CLASSPATH=$(hadoop classpath)


报错查看

cd /tmp/wuss/
tail -n 300 hive.log
或
tail -f hive.log

cd /opt/module/hadoop-3.1.3/etc/hadoop/
```
## 4. 测试
```shell
启动
hive
create table student(id int, name string);
insert into table student values(1,'abc');

报错查看
hive  --hiveconf  hive.root.logger=DEBUG,console  -e "insert into table student values(1,'abc');"
可能的解决办法:
加大内存,在hadoop102:9870中查看datanode,若是没有三个活着的节点,则在三个hadoop里的/opt/module/hadoop-3.1.3/data/dfs/data/current里修改VERSION里的datanodeUuid改为不同
然后在hadoop102上 
hdfs namenode -format
然后重新配置hive-on-spark

insert into table student values(2,'qwe');
insert into table student values(3,'tyu');
```