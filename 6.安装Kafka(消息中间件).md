## 1. 安装
```shell
cd /opt/software/
上传kafka_2.11-2.4.1.tgz
tar -zxvf kafka_2.11-2.4.1.tgz -C ../module/
mkdir /opt/module/kafka_2.11-2.4.1/datas
vim /opt/module/kafka_2.11-2.4.1/config/server.properties

---------------------------------------------------
#修改
broker.id=0
#为
broker.id=2  #这里2和zookeeper那里的一样
---------------------------------------------------
#修改
log.dirs=/tmp/kafka-logs
#为
log.dirs=/opt/module/kafka_2.11-2.4.1/datas
---------------------------------------------------
#修改
zookeeper.connect=localhost:2181
#为
zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka
---------------------------------------------------

#环境变量
sudo vim /etc/profile.d/my_env.sh
#添加

#kafka
export KAFKA_HOME=/opt/module/kafka_2.11-2.4.1
export PATH=$PATH:$KAFKA_HOME/bin

#断开重连
#zkS看能不能Tab补全,能就成功了

#分发
cd /opt/module
xsync kafka_2.11-2.4.1/

#分发环境配置
scp /etc/profile.d/my_env.sh root@hadoop103:/etc/profile.d/
scp /etc/profile.d/my_env.sh root@hadoop104:/etc/profile.d/

#断开重连
vim /opt/module/kafka_2.11-2.4.1/config/server.properties

#hadoop103写3,每台机不同
broker.id=3
#hadoop104写4,每台机不同
broker.id=4

#启动之前必须启动zookeeper
#启动
kafka-server-start.sh -daemon /opt/module/kafka_2.11-2.4.1/config/server.properties
#停止
kafka-server-stop.sh
```
## 2. 脚本
```shell
cd /home/wuss/bin
vim kafka.sh
i

#!/bin/bash
if [ $# -lt 1 ]
then
  echo "USAGE: kafka.sh {start|stop}"
  exit
fi  
case $1 in
start)
	for i in hadoop102 hadoop103 hadoop104
	do
	  echo "=================> START $i KF <================="
	  ssh $i /opt/module/kafka_2.11-2.4.1/bin/kafka-server-start.sh -daemon /opt/module/kafka_2.11-2.4.1/config/server.properties
	done
;;
stop)
	for i in hadoop102 hadoop103 hadoop104
	do
	  echo "=================> STOP $i KF <================="
	  ssh $i /opt/module/kafka_2.11-2.4.1/bin/kafka-server-stop.sh
	done
;;

*)
  echo "USAGE: kafka.sh {start|stop}"
  exit
;;
esac

chmod 777 kafka.sh
停止kafka后才能停止zookeeper
``` 

## 3. Kafka常用命令
```shell
#以下命令均可在任意位置使用

kafka.sh start

#1.查看Kafka Topic列表
kafka-topics.sh --bootstrap-server hadoop102:9092 --list

#2.创建Kafka Topic
#进入到/opt/module/kafka_2.11-2.4.1/目录下创建日志主题
kafka-topics.sh --bootstrap-server hadoop102:9092 --create --topic first
kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first

kafka-topics.sh --bootstrap-server hadoop102:9092 --create --replication-factor 2 --partitions 3 --topic first1
kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first1

kafka-topics.sh --bootstrap-server hadoop102:9092 --list
#3.查看Kafka Topic详情

kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic first

#4.删除Kafka Topic
kafka-topics.sh --bootstrap-server hadoop102:9092 --delete --topic first

#5.Kafka生产消息
kafka-console-producer.sh --topic first --broker-list hadoop102:9092

#6.Kafka消费消息
kafka-console-consumer.sh --topic first --bootstrap-server hadoop102:9092

kafka-console-consumer.sh --topic first --bootstrap-server hadoop102:9092 --from-beginning

--from-beginning：会把主题中以往所有的数据都读取出来。根据业务场景选择是否增加该配置。


```
## 4. Kafka压力测试(可跳过)
```shell
cd /opt/module/kafka_2.11-2.4.1/bin

#Kafka Producer压力测试
kafka-producer-perf-test.sh --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:2181,hadoop103:2181,hadoop104:2181

#Kafka Consumer压力测试
kafka-consumer-perf-test.sh --broker-list hadoop102:2181,hadoop103:2181,hadoop104:2181 --topic test --fetch-size 10000 --messages 500000 --threads 1
```